{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9f5da39-cca3-48eb-b7e9-1a172e143890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bce8de99-ef2f-472c-bf2c-b74774e81df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition(x:torch.tensor,window_size:int):\n",
    "    B,C,H,W = x.shape\n",
    "    x = x.view(B,H//window_size,window_size,W//window_size,window_size,3)\n",
    "    return x.permute(0,1,3,2,4,5).contiguous().view(-1,win,win,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "257d7f8c-ea03-480f-be7b-7f170b11caec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,patch_size=4,C=96):\n",
    "        super().__init__()\n",
    "        self.linear_embed = nn.Conv2d(3,C,kernel_size=patch_size,stride=patch_size)\n",
    "        self.ln = nn.LayerNorm(C)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.ln(rearrange(self.linear_embed(x),'b c h w -> b (h w) c')))\n",
    "        return x\n",
    "\n",
    "Embedding()(torch.randn(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f63cd84-9384-40ef-9ba5-573617376c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 192])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MergePatch(nn.Module):\n",
    "    def __init__(self,C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4*C, 2*C)\n",
    "        self.ln = nn.LayerNorm(2*C)\n",
    "    def forward(self,x):\n",
    "        h = w = int(math.sqrt(x.shape[1])/2)\n",
    "        x = rearrange(x,'b (h s1 w s2) c -> b (h w) (s1 s2 c)',s1=2,s2=2,h=h,w=w)\n",
    "        return self.ln(self.linear(x))\n",
    "\n",
    "MergePatch(96)(torch.randn(1,3136,96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d23ba0c6-6ac7-40ba-b14f-970b3c9c113d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionalEmbedding(nn.Module):\n",
    "    def __init__(self,window_size):\n",
    "        super().__init__()\n",
    "        self.B = nn.Parameter(torch.zeros(2 * window_size - 1, 2 * window_size - 1))\n",
    "        idx = torch.arange(window_size)\n",
    "        idx = torch.stack([torch.meshgrid(idx,idx)[0].flatten(),torch.meshgrid(idx,idx)[1].flatten()])\n",
    "        idx = idx[:,None,:] - idx[:,:,None]\n",
    "        self.embeddings = nn.Parameter((self.B[idx[0,:,:],idx[1,:,:]]),requires_grad=False)\n",
    "    def forward(self,x):\n",
    "        return x+self.embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39a8286c-31c8-478a-bf6a-fc5371b6389a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ShiftedWindowAttention(nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,window_size=7,attn_dropout=0.2,ffd_dropout=0.2,mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mask = mask\n",
    "        self.window_size = window_size\n",
    "        self.linear = nn.Linear(embed_dim,3 * embed_dim)\n",
    "        self.ffd = nn.Linear(embed_dim,embed_dim)\n",
    "        self.relative_pos_embed = RelativePositionalEmbedding(window_size=window_size)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.ffd_dropout = nn.Dropout(ffd_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        h = w = int(math.sqrt(x.shape[1]))\n",
    "        x = self.linear(x)\n",
    "        x = rearrange(x,'b (h w) (c k) -> b h w c k',k=3,h=h,w=w)\n",
    "        \n",
    "        if self.mask:\n",
    "            x = torch.roll(x,(-self.window_size//2,-self.window_size//2),dims=(1,2))\n",
    "        \n",
    "        x = rearrange(x,'b (h m1) (w m2) (nh he) k -> b nh h w (m1 m2) he k',nh=self.num_heads,he=head_dim,m1=self.window_size,m2=self.window_size)\n",
    "        \n",
    "        Q,K,V = x.chunk(3,dim=6)\n",
    "        Q,K,V = map(lambda x : x.squeeze(-1),[Q,K,V])\n",
    "        w = (Q @ K.transpose(4,5))/math.sqrt(head_dim)\n",
    "        w = self.relative_pos_embed(w)\n",
    "        if self.mask:\n",
    "            row_mask = torch.zeros((self.window_size**2,self.window_size**2)).to(device)\n",
    "            row_mask[-self.window_size * (self.window_size//2):,:-self.window_size * (self.window_size//2)] = float('-inf')\n",
    "            row_mask[:-self.window_size * (self.window_size//2),-self.window_size * (self.window_size//2):] = float('-inf')\n",
    "            column_mask = rearrange(row_mask,'(r w1) (c w2) -> (w1 r) (w2 c)', w1=self.window_size, w2=self.window_size).to(device)\n",
    "            w[:,:,-1,:] += row_mask\n",
    "            w[:,:,:,-1] += column_mask\n",
    "        \n",
    "        attention = F.softmax(w,dim=-1) @ V\n",
    "        attention = self.attn_dropout(attention)\n",
    "        x = rearrange(attention,'b nh h w (m1 m2) he -> b (h m1) (w m2) (nh he)',m1=self.window_size,m2=self.window_size)\n",
    "        \n",
    "        if self.mask:\n",
    "            x = torch.roll(x,(self.window_size//2,self.window_size//2),dims=(1,2))\n",
    "        \n",
    "        x = rearrange(x,'b h w c -> b (h w) c')\n",
    "        \n",
    "        return self.ffd_dropout(self.ffd(x))\n",
    "ShiftedWindowAttention(embed_dim=96,num_heads=4,window_size=7,mask=False)(torch.randn(1,56 * 56,96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7832d3d1-3a76-42c9-b303-0a6e96ecf417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,mask,window_size=7):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.WMSA = ShiftedWindowAttention(embed_dim=embed_dim,num_heads=num_heads,mask=mask,window_size=window_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim,embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*4,embed_dim),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        wmsa = self.dropout(self.WMSA(self.ln(x)) + x)\n",
    "        x = self.dropout(self.mlp(self.ln(wmsa)) + wmsa)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,window_size=7):\n",
    "        super().__init__()\n",
    "        self.wmsa = SwinBlock(embed_dim,num_heads,mask=False)\n",
    "        self.swmsa = SwinBlock(embed_dim,num_heads,mask=True)\n",
    "    def forward(self,x):\n",
    "        return self.swmsa(self.wmsa(x))\n",
    "\n",
    "SwinTransformerBlock(embed_dim=96,window_size=6,num_heads=4)(torch.randn(1,56*56,96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20af1f01-61a8-4f53-a169-04ac4edc80a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(patch_size=4,C=96)\n",
    "        self.stage1 = nn.Sequential(\n",
    "            SwinTransformerBlock(embed_dim=96,num_heads=3),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            MergePatch(96),\n",
    "            SwinTransformerBlock(192,6),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            MergePatch(192),\n",
    "            SwinTransformerBlock(384,12),\n",
    "            SwinTransformerBlock(384,12),\n",
    "            SwinTransformerBlock(384,12),\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            MergePatch(384),\n",
    "            SwinTransformerBlock(768,24),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        return x\n",
    "SwinTransformer()(torch.randn(1,3,224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b12294b-8e13-4c07-bc8b-39d15bd415ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27523992"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = SwinTransformer()\n",
    "sum([p.numel() for p in model_.parameters()]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
